# Temporal-Aware Multi-Task Transformer for Finance

A PyTorch implementation of a Transformer-based model designed for financial time-series and textual data. This project features temporal-aware embeddings (Time2Vec) and a multi-task learning architecture to predict market returns, volatility, events, and regimes simultaneously.

## ðŸš€ Key Features

*   **Temporal Awareness**: Uses `Time2Vec` for continuous time encoding and `CalendarEmbeddings` for discrete time features (Hour, Day, Weekday, Month).
*   **Multi-Modal Fusion**: Incorporates a `Variable Selection Network` (inspired by TFT) to dynamically weigh numerical (price/volume) and textual (news embeddings) inputs.
*   **Multi-Task Learning**: Shared Transformer backbone with 4 specialized heads:
    *   **Return**: Price movement prediction (Regression).
    *   **Volatility**: Market risk estimation (Regression, Softplus activation).
    *   **Event Impact**: News significance score (Sigmoid activation).
    *   **Regime**: Market state classification (Bull/Bear/Shock).
*   **Synthetic Data Pipeline**: Built-in generator to create realistic financial sequences with injected "shocks" and correlated text embeddings.

## ðŸ›  Setup Instructions

### 1. Conda Environment (Recommended)

```bash
# Create a new conda environment
conda create -n tft-finance python=3.10 -y
conda activate tft-finance

# Install dependencies
pip install -r finance_transformer/requirements.txt
```

### 2. Standard Pip install

```bash
pip install torch pandas numpy scikit-learn
```

## ðŸ“ˆ How to Run

### Generate Data & Train
The `main.py` script handles data generation, training, and evaluation.

```bash
python finance_transformer/main.py
```

Upon running, the script will:
1.  Generate 2,000 synthetic samples.
2.  Save raw sequence data to `finance_transformer/data/synthetic_data.csv`.
3.  Execute a 10-epoch training loop.
4.  Print sample predictions vs. ground truth for all 4 tasks.

## ðŸ“‚ Project Structure

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ finance_transformer/
â”‚   â”œâ”€â”€ main.py              # Entry point: Training loop and evaluation
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ generator.py     # Synthetic data generation logic
â”‚   â”‚   â””â”€â”€ synthetic_data.csv # Generated data (after first run)
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ transformer.py   # Main model assembly
â”‚   â”‚   â”œâ”€â”€ embeddings.py    # Time2Vec and Calendar embeddings
â”‚   â”‚   â”œâ”€â”€ layers.py        # GRN and Variable Selection logic
â”‚   â”‚   â””â”€â”€ heads.py         # Multi-task output heads
â”‚   â””â”€â”€ docs/
â”‚       â”œâ”€â”€ architecture.md  # Detailed explanation of temporal/multi-task concepts
â”‚       â””â”€â”€ plan.md          # Original implementation plan
```

## ðŸ“– Documentation

For a deeper dive into the methodology:
- [Architecture Concepts](finance_transformer/docs/architecture.md): Understanding Temporal Awareness and GRNs.
- [Implementation Plan](finance_transformer/docs/plan.md): The technical roadmap used to build this project.
